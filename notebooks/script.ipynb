{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import csv\n",
    "import spacy\n",
    "from enum import Enum\n",
    "from spacy import displacy\n",
    "from spacy.symbols import PROPN, NOUN, CCONJ, ADP, VERB\n",
    "import numpy as np\n",
    "import speech_recognition as sr \n",
    "from scipy.sparse import csr_matrix\n",
    "from sknetwork.path import shortest_path\n",
    "\n",
    "from collections import defaultdict\n",
    "import functools\n",
    "import itertools\n",
    "\n",
    "currentPath = os.path.dirname(os.path.abspath(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Read 3150 go and back trips (1575 distinc trips) out of 1575 rows for 817 distinct train stations.\n"
     ]
    }
   ],
   "source": [
    "# Reading trips from csv file\n",
    "# Inspired from https://stackoverflow.com/a/12398967\n",
    "\n",
    "pathCount = 0\n",
    "timeTableFileName = os.path.join(currentPath, './data/timetables_edited.csv')\n",
    "\n",
    "# Create dictionary to associates a station name with an id\n",
    "trainStationNameToId = defaultdict(functools.partial(next, itertools.count()))\n",
    "\n",
    "with open(timeTableFileName, newline='', encoding='UTF-8') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "    # Set of the reading position (ignoring header line)\n",
    "    csvfile.seek(0)\n",
    "    next(reader)\n",
    "\n",
    "    # First reading to get the number different stations and init shape of trips object\n",
    "    for row in reader:\n",
    "        idxA = trainStationNameToId[row[1]]\n",
    "        idxB = trainStationNameToId[row[2]]\n",
    "    numberOfTrainstations = len(trainStationNameToId)\n",
    "    trips = np.zeros((numberOfTrainstations, numberOfTrainstations))\n",
    "\n",
    "    # Reset of the reading position (ignoring header line)\n",
    "    csvfile.seek(0)\n",
    "    next(reader)\n",
    "\n",
    "    # Reading data\n",
    "    for row in reader:\n",
    "        pathCount += 1 \n",
    "        idxA = trainStationNameToId[row[1]]\n",
    "        idxB = trainStationNameToId[row[2]]\n",
    "\n",
    "        # If trip already exist/has already be read, display message\n",
    "        indexTupple = (idxA, idxB) if idxA < idxB else (idxB, idxA)\n",
    "        if trips[indexTupple] != 0:\n",
    "            print(f\"Trip {row[1]} - {row[2]} with a distance of {row[3]} has already be read with a distance of {trips[indexTupple]}. Ignoring the new one.\")\n",
    "        else:\n",
    "            trips[indexTupple] = int(row[3])\n",
    "\n",
    "\n",
    "# Create dictionarry to map an id to its train station name\n",
    "trainStationIdToName = dict((id, name) for name, id in trainStationNameToId.items())\n",
    "\n",
    "# Make matrix symetrical as the trips are not directed but can be taken in both directions\n",
    "# Source from https://stackoverflow.com/a/42209263\n",
    "i_lower = np.tril_indices(numberOfTrainstations, -1)\n",
    "trips[i_lower] = trips.T[i_lower]\n",
    "\n",
    "# Creating Compressed Sparse Row (CSR) matrix to store and work more efficiently with only the trips\n",
    "tripGraph = csr_matrix(trips)\n",
    "\n",
    "print(f\"Read {tripGraph.getnnz()} go and back trips ({int(tripGraph.getnnz() / 2)} distinc trips) out of {pathCount} rows for {len(trainStationNameToId)} distinct train stations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trip:\n",
    "    def __init__(self, startStationId, endStationId, path, totalDuration):\n",
    "        self.startStationId = startStationId\n",
    "        self.endStationId = endStationId\n",
    "        self.path = path\n",
    "        if totalDuration is None:\n",
    "            self.totalDuration = None\n",
    "        else:\n",
    "            self.totalDuration = int(totalDuration)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Trip from {trainStationIdToName[self.startStationId]} to {trainStationIdToName[self.endStationId]} for a total duration of {self.totalDuration} minutes by this path: {self.pathToString()}\"\n",
    "\n",
    "    def pathToString(self):\n",
    "        string = \"\"\n",
    "        for i in range(len(self.path)):\n",
    "            if i > 0:\n",
    "                string = string + \" -> \"\n",
    "            string = string + trainStationIdToName[self.path[i]]\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#1 - Trip from Gare de Orléans to Gare de Paris-Austerlitz for a total duration of 75 minutes by this path: Gare de Orléans -> Gare de Paris-Austerlitz\n",
      "Gare de Orléans\n",
      "Gare de Paris-Austerlitz\n",
      "#2 - Trip from Gare de Paris-Est to Gare de Strasbourg for a total duration of 287 minutes by this path: Gare de Paris-Est -> Gare de Epernay -> Gare de Metz-Ville -> Gare de Strasbourg\n",
      "Gare de Paris-Est\n",
      "Gare de Epernay\n",
      "Gare de Metz-Ville\n",
      "Gare de Strasbourg\n",
      "#3 - Could not find one or both station of the sub-trip\n",
      "#4 - Could not find one or both station of the sub-trip\n",
      "#5 - Trip from Gare de Mulhouse to Gare de Mulhouse for a total duration of 0 minutes by this path: Gare de Mulhouse -> Gare de Mulhouse\n",
      "Gare de Mulhouse\n",
      "Gare de Mulhouse\n"
     ]
    }
   ],
   "source": [
    "# Functions used to determine the shortest path between cities \n",
    "\n",
    "def getPathBetweenIds(trainStationStartIds: list, trainStationEndIds: list):\n",
    "    global tripGraph\n",
    "    paths = []\n",
    "    # If start array contains one element also contained in end array -> return path from/to the same station\n",
    "    for startId in trainStationStartIds:\n",
    "        if startId in trainStationEndIds:\n",
    "            return [int(startId), int(startId)]\n",
    "\n",
    "    # As shortest_path() does not support multiple sources and multiple targets at the same time, we'll iterate through all start points and manually concat the results\n",
    "    if(len(trainStationStartIds) > 1 and len(trainStationEndIds) > 1):\n",
    "        for trainStationEndId in trainStationEndIds:\n",
    "            results = shortest_path(tripGraph, sources=[int(i) for i in trainStationStartIds], targets=[int(trainStationEndId)], method='D')\n",
    "            for result in results:\n",
    "                if len(result) >= 2:\n",
    "                    paths.append(result)\n",
    "        return paths\n",
    "    else:\n",
    "        results = shortest_path(tripGraph, sources=[int(i) for i in trainStationStartIds], targets=[int(i) for i in trainStationEndIds], method='D')\n",
    "        for result in results:\n",
    "            if len(result) >= 2:\n",
    "                paths.append(result)\n",
    "        return paths\n",
    "\n",
    "def isCityMatchingKey(city: str, key: str):\n",
    "    return city.lower() in key.lower()\n",
    "\n",
    "def getPathBetweenCities(start: str, end: str):\n",
    "    trainStationStartIds = np.array([])\n",
    "    trainStationEndIds = np.array([])\n",
    "    \n",
    "    # Get all stations that contains the searched name\n",
    "    for key, value in trainStationNameToId.items():\n",
    "        if isCityMatchingKey(start, key):\n",
    "            trainStationStartIds = np.append(trainStationStartIds, value)\n",
    "        if isCityMatchingKey(end, key):\n",
    "            trainStationEndIds = np.append(trainStationEndIds, value)\n",
    "\n",
    "    if len(trainStationStartIds) > 0 and len(trainStationEndIds) > 0:\n",
    "        return getPathBetweenIds(trainStationStartIds, trainStationEndIds)\n",
    "    else:\n",
    "        return np.array([])\n",
    "\n",
    "def getBestPathForFullTrip(tripCityWaypoints: list):\n",
    "    global tripGraph\n",
    "    fullTrip = np.array(np.zeros(len(tripCityWaypoints)-1), dtype=object)\n",
    "    # Iterate through all sub trips\n",
    "    for trip in range(len(fullTrip)):\n",
    "        paths = getPathBetweenCities(tripCityWaypoints[trip], tripCityWaypoints[trip+1])\n",
    "        minDistance = None\n",
    "        keptPath = None\n",
    "        startId = None\n",
    "        endId = None\n",
    "\n",
    "        # Iterate though the returned array\n",
    "        for path in paths:\n",
    "            distance = 0\n",
    "            # Nested array => multiple start/end possible\n",
    "            if isinstance(path, list):\n",
    "                for i in range(len(path)-1):\n",
    "                    distance = distance + tripGraph[(path[i], path[i+1])]\n",
    "                if minDistance is None or distance < minDistance:\n",
    "                    minDistance = distance\n",
    "                    keptPath = path\n",
    "                    startId = path[0]\n",
    "                    endId = path[len(path)-1]\n",
    "            \n",
    "            # Scalar value => only one path possible\n",
    "            else:\n",
    "                for i in range(len(paths)-1):\n",
    "                    distance = distance + tripGraph[(paths[i], paths[i+1])]\n",
    "                minDistance = distance\n",
    "                keptPath = paths\n",
    "                startId = paths[0]\n",
    "                endId = paths[len(paths)-1]\n",
    "\n",
    "        fullTrip[trip] = Trip(startId, endId, keptPath, minDistance)\n",
    "    return fullTrip\n",
    "\n",
    "\n",
    "# DEBUG\n",
    "# Use following to test pathfinding functions above\n",
    "def testPathfinding():\n",
    "    bestTrips = getBestPathForFullTrip(['Orléan', 'Paris', 'Strasbourg', 'dsfdsfd', 'Mulhouse', 'Mulhouse'])\n",
    "    for i in range(len(bestTrips)):\n",
    "        if bestTrips[i].path is not None:\n",
    "            print(f\"#{i+1} - {bestTrips[i]}\")\n",
    "        else:\n",
    "            if bestTrips[i].startStationId is None or bestTrips[i].endStationId is None:\n",
    "                print(f\"#{i+1} - Could not find one or both station of the sub-trip\")\n",
    "            else:\n",
    "                print(f\"#{i+1} - Could not find a path between the both stations\")\n",
    "testPathfinding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Adjusting to noise level...\n",
      "Listening...\n",
      "Voice recognition...\n",
      "Parsed: 'un smoothie à la banane et une pizza 4 fromages s'il vous plaît'\n"
     ]
    }
   ],
   "source": [
    "# Getting user's request\n",
    "# Inspired from https://www.geeksforgeeks.org/python-convert-speech-to-text-and-text-to-speech/\n",
    "try: \n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source: \n",
    "        print(\"Adjusting to noise level...\")\n",
    "        r.adjust_for_ambient_noise(source, duration=0.2) \n",
    " \n",
    "        print(\"Listening...\")\n",
    "        audio = r.listen(source) \n",
    "            \n",
    "        print(\"Voice recognition...\")\n",
    "        userRequest = r.recognize_google(audio, language=\"fr-FR\") \n",
    "\n",
    "        print(f\"Parsed: '{userRequest}'\") \n",
    "            \n",
    "except sr.RequestError as e: \n",
    "    print(f\"Exception during request parsing: {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Request: J'aimerais aller d'Orléans à Paris puis dans les Vosges\n",
      "Locations found: ['Orléans', 'Paris', 'Vosges']\n",
      "Token #1 : Orléans\n",
      "Found ADP: de - STRONG\n",
      "Found VERB: aller - STRONG\n",
      "Using: de\n",
      "---------------\n",
      "Token #2 : Paris\n",
      "Found ADP: à - WEAK\n",
      "Found VERB: aller - STRONG\n",
      "Using: aller\n",
      "---------------\n",
      "Token #3 : vosge\n",
      "Found CCONJ: puis - STRONG\n",
      "Using: puis\n",
      "---------------\n",
      "Result trip: ['Orléans', 'Paris', 'Vosges']\n",
      "\n",
      "\n",
      "\n",
      "***************************    # 0    ***************************\n",
      "result:    ['Orléans', 'Paris', 'Vosges']\n",
      "exprected: ['Orléans', 'Paris', 'Vosges']\n",
      "*****************************************************************\n",
      "\n",
      "\n",
      "\n",
      "Request: Je veux aller à Marseille à partir de Lyon\n",
      "Locations found: ['Marseille', 'Lyon']\n",
      "Token #1 : marseille\n",
      "Found ADP: à - WEAK\n",
      "Found VERB: aller - STRONG\n",
      "Using: aller\n",
      "---------------\n",
      "Token #2 : Lyon\n",
      "Found ADP: de - STRONG\n",
      "Using: de\n",
      "---------------\n",
      "Result trip: ['Lyon', 'Marseille']\n",
      "\n",
      "\n",
      "\n",
      "***************************    # 1    ***************************\n",
      "result:    ['Lyon', 'Marseille']\n",
      "exprected: ['Lyon', 'Marseille']\n",
      "*****************************************************************\n",
      "\n",
      "\n",
      "\n",
      "Request: Je veux visiter Paris en partant de Bordeaux et en passant par Nantes\n",
      "Locations found: ['Paris', 'Bordeaux', 'Nantes']\n",
      "Token #1 : Paris\n",
      "Found VERB: visiter - STRONG\n",
      "Using: visiter\n",
      "---------------\n",
      "Token #2 : bordeaux\n",
      "Found ADP: de - STRONG\n",
      "Using: de\n",
      "---------------\n",
      "Token #3 : nante\n",
      "Found ADP: par - WEAK\n",
      "Found VERB: passer - WEAK\n",
      "Using: par\n",
      "---------------\n",
      "Result trip: ['Bordeaux', 'Nantes', 'Paris']\n",
      "\n",
      "\n",
      "\n",
      "***************************    # 2    ***************************\n",
      "result:    ['Bordeaux', 'Nantes', 'Paris']\n",
      "exprected: ['Bordeaux', 'Nantes', 'Paris']\n",
      "*****************************************************************\n",
      "\n",
      "\n",
      "\n",
      "Request: Je veux prendre le train à Mulhouse à destination de Strasbourg\n",
      "Locations found: ['Mulhouse', 'Strasbourg']\n",
      "Token #1 : Mulhouse\n",
      "Found ADP: à - WEAK\n",
      "Using: à\n",
      "---------------\n",
      "Token #2 : Strasbourg\n",
      "Found NOUN: destination - WEAK\n",
      "Using: destination\n",
      "---------------\n",
      "Result trip: ['Mulhouse', 'Strasbourg']\n",
      "\n",
      "\n",
      "\n",
      "***************************    # 3    ***************************\n",
      "result:    ['Mulhouse', 'Strasbourg']\n",
      "exprected: ['Mulhouse', 'Strasbourg']\n",
      "*****************************************************************\n",
      "\n",
      "\n",
      "\n",
      "Request: Strasbourg en provenance de Mulhouse\n",
      "Locations found: ['Strasbourg', 'Mulhouse']\n",
      "Token #1 : strasbourg\n",
      "Using default weight\n",
      "Using: default\n",
      "---------------\n",
      "Token #2 : Mulhouse\n",
      "Found NOUN: provenance - STRONG\n",
      "Using: provenance\n",
      "---------------\n",
      "Result trip: ['Mulhouse', 'Strasbourg']\n",
      "\n",
      "\n",
      "\n",
      "***************************    # 4    ***************************\n",
      "result:    ['Mulhouse', 'Strasbourg']\n",
      "exprected: ['Mulhouse', 'Strasbourg']\n",
      "*****************************************************************\n",
      "\n",
      "\n",
      "\n",
      "Request: Je veux aller de Mulhouse à Strasbourg\n",
      "Locations found: ['Mulhouse', 'Strasbourg']\n",
      "Token #1 : Mulhouse\n",
      "Found ADP: de - STRONG\n",
      "Found VERB: aller - STRONG\n",
      "Using: de\n",
      "---------------\n",
      "Token #2 : Strasbourg\n",
      "Found ADP: à - WEAK\n",
      "Found VERB: aller - STRONG\n",
      "Using: aller\n",
      "---------------\n",
      "Result trip: ['Mulhouse', 'Strasbourg']\n",
      "\n",
      "\n",
      "\n",
      "***************************    # 5    ***************************\n",
      "result:    ['Mulhouse', 'Strasbourg']\n",
      "exprected: ['Mulhouse', 'Strasbourg']\n",
      "*****************************************************************\n",
      "\n",
      "\n",
      "\n",
      "Request: Je veux faire Paris Gare De l'est Marseille\n",
      "Locations found: [\"Paris Gare De l'\", 'Marseille']\n",
      "Token #1 : Paris\n",
      "Using default weight\n",
      "Using: default\n",
      "---------------\n",
      "Token #2 : marseill\n",
      "Using default weight\n",
      "Using: default\n",
      "---------------\n",
      "Result trip: ['Paris', 'Marseille']\n",
      "\n",
      "\n",
      "\n",
      "***************************    # 6    ***************************\n",
      "result:    ['Paris', 'Marseille']\n",
      "exprected: ['Paris', 'Marseille']\n",
      "*****************************************************************\n",
      "\n",
      "\n",
      "\n",
      "Request: Je veux aller à Paris après être allé à Mulhouse depuis Lyon\n",
      "Locations found: ['Paris', 'Mulhouse', 'Lyon']\n",
      "Token #1 : Paris\n",
      "Found ADP: à - WEAK\n",
      "Found VERB: aller - STRONG\n",
      "Using: aller\n",
      "---------------\n",
      "Token #2 : Mulhouse\n",
      "Found ADP: à - WEAK\n",
      "Found VERB_MARK: après - STRONG\n",
      "Using: après\n",
      "---------------\n",
      "Token #3 : Lyon\n",
      "Found ADP: depuis - STRONG\n",
      "Found VERB_MARK: après - STRONG\n",
      "Using: depuis\n",
      "---------------\n",
      "Result trip: ['Lyon', 'Mulhouse', 'Paris']\n",
      "\n",
      "\n",
      "\n",
      "***************************    # 7    ***************************\n",
      "result:    ['Lyon', 'Mulhouse', 'Paris']\n",
      "exprected: ['Lyon', 'Mulhouse', 'Paris']\n",
      "*****************************************************************\n",
      "\n",
      "\n",
      "\n",
      "Request: Paris-Marseille\n",
      "Locations found: ['Paris', 'Marseille']\n",
      "Token #1 : Paris\n",
      "Using default weight\n",
      "Using: default\n",
      "---------------\n",
      "Token #2 : Marseille\n",
      "Using default weight\n",
      "Using: default\n",
      "---------------\n",
      "Result trip: ['Paris', 'Marseille']\n",
      "\n",
      "\n",
      "\n",
      "***************************    # 8    ***************************\n",
      "result:    ['Paris', 'Marseille']\n",
      "exprected: ['Paris', 'Marseille']\n",
      "*****************************************************************\n",
      "\n",
      "\n",
      "\n",
      "Request: Je suis à Paris et je veux aller à Strasbourg avec mon amis Frank que je récupère à Mulhouse\n",
      "Locations found: ['Paris', 'Strasbourg', 'Mulhouse']\n",
      "Token #1 : Paris\n",
      "Found ADP: à - WEAK\n",
      "Using: à\n",
      "---------------\n",
      "Token #2 : Strasbourg\n",
      "Found ADP: à - WEAK\n",
      "Found VERB: aller - STRONG\n",
      "Using: aller\n",
      "---------------\n",
      "Token #3 : Mulhouse\n",
      "Found ADP: à - WEAK\n",
      "Using: à\n",
      "---------------\n",
      "Result trip: ['Paris', 'Mulhouse', 'Strasbourg']\n",
      "\n",
      "\n",
      "\n",
      "***************************    # 9    ***************************\n",
      "result:    ['Paris', 'Mulhouse', 'Strasbourg']\n",
      "exprected: ['Paris', 'Mulhouse', 'Strasbourg']\n",
      "*****************************************************************\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract cities list in the right travel order from the user's request\n",
    "\n",
    "# Spacy documentation: https://spacy.io/usage/linguistic-features\n",
    "# NLP Feature that we use or can be useful:\n",
    "#   - Lemmatization: we use it to avoid defining every possible form of a word when defining our rules\n",
    "#   - PoS Tagging: we use it to detect the function of a word which helps us to choose which rule we apply and reduce the number of checks we need to do\n",
    "#   - Dependency Parsing: we use it to navigate in the sentence between the words that are linked\n",
    "#   - Word Senses: we use it to define our rules that assigns a specific direction and strength to apply to certain words/fixed word groups\n",
    "#   - Constituent Parsing: we currently don't use it, but it might help us improve reliability by splitting sentences into sub-sentences that we can then organize between them\n",
    "\n",
    "\n",
    "class RelationDirection(Enum):\n",
    "    NONE = 1\n",
    "    START = 2\n",
    "    DEST = 3\n",
    "\n",
    "class RelationStrength(Enum):\n",
    "    NONE = 1\n",
    "    WEAK = 2\n",
    "    STRONG = 3\n",
    "\n",
    "\n",
    "class WordSense:\n",
    "    def __init__(self, word: str, direction: RelationDirection, strength: RelationStrength):\n",
    "        self.word = word\n",
    "        self.direction = direction\n",
    "        self.strength = strength\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Word '{self.word}' has a direction of {self.direction.name} and a {self.strength.name} strength.\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Word '{self.word}' has a direction of {self.direction.name} and a {self.strength.name} strength.\"\n",
    "\n",
    "class LinkedWordSense:\n",
    "    def __init__(self, word: str, fixedWord: str, direction: RelationDirection, strength: RelationStrength):\n",
    "        self.word = word\n",
    "        self.fixedWord = fixedWord\n",
    "        self.direction = direction\n",
    "        self.strength = strength\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Words '{self.word}' fixed with '{self.fixedWord}' has a direction of {self.direction.name} and a {self.strength.name} strength.\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Words '{self.word}' fixed with '{self.fixedWord}' has a direction of {self.direction.name} and a {self.strength.name} strength.\"\n",
    "\n",
    "# CCONJ links: 'cc'_child\n",
    "CCONJ_Relation = [\n",
    "    # Start\n",
    "    WordSense(\"depuis\",     RelationDirection.START, RelationStrength.STRONG),\n",
    "    # Destination\n",
    "    WordSense(\"puis\",       RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    WordSense(\"et\",         RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    WordSense(\"enfin\",      RelationDirection.DEST,  RelationStrength.STRONG)\n",
    "]\n",
    "\n",
    "# NOUN links: 'nmod'_parent\n",
    "NOUN_Relation = [\n",
    "    # Start\n",
    "    WordSense(\"provenance\",     RelationDirection.START, RelationStrength.STRONG),\n",
    "    # Destination\n",
    "    WordSense(\"direction\",      RelationDirection.DEST,  RelationStrength.WEAK),\n",
    "    WordSense(\"destination\",    RelationDirection.DEST,  RelationStrength.WEAK)\n",
    "]\n",
    "\n",
    "# ADP_FIXED has the priority \n",
    "# ADP links: 'case'_child, 'dep'_parent\n",
    "ADP_FIXED_Relation = [\n",
    "    # Start\n",
    "    LinkedWordSense(\"à\",\"partir\",       RelationDirection.START, RelationStrength.STRONG),\n",
    "    LinkedWordSense(\"en\", \"partant\",    RelationDirection.START, RelationStrength.STRONG),\n",
    "    # Destination\n",
    "    LinkedWordSense(\"à\",\"destination\",  RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    LinkedWordSense(\"en\",\"direction\",   RelationDirection.DEST,  RelationStrength.WEAK)\n",
    "]\n",
    "ADP_Relation = [\n",
    "    # Start\n",
    "    WordSense(\"de\",     RelationDirection.START, RelationStrength.STRONG),\n",
    "    WordSense(\"du\",     RelationDirection.START, RelationStrength.STRONG),\n",
    "    WordSense(\"des\",    RelationDirection.START, RelationStrength.STRONG),\n",
    "    WordSense(\"depuis\", RelationDirection.START, RelationStrength.STRONG),\n",
    "    # Destination\n",
    "    WordSense(\"à\",      RelationDirection.DEST,  RelationStrength.WEAK),\n",
    "    WordSense(\"au\",     RelationDirection.DEST,  RelationStrength.WEAK),\n",
    "    WordSense(\"aux\",    RelationDirection.DEST,  RelationStrength.WEAK),\n",
    "    WordSense(\"dans\",   RelationDirection.DEST,  RelationStrength.WEAK),\n",
    "    WordSense(\"en\",     RelationDirection.DEST,  RelationStrength.WEAK),\n",
    "    WordSense(\"par\",    RelationDirection.DEST,  RelationStrength.WEAK) # par : \"passer par Paris\"\n",
    "] \n",
    "\n",
    "# VERB links: 'obl:arg'_parent, 'obl:mod'_parent\n",
    "# \"partir\" is ambiguous: \"partir de ...\" \"partir à ...\"\n",
    "VERB_MARK_Relation = [\n",
    "    WordSense(\"après\",   RelationDirection.START, RelationStrength.STRONG),\n",
    "    WordSense(\"avant\",   RelationDirection.DEST, RelationStrength.STRONG)\n",
    "]\n",
    "VERB_Relation = [\n",
    "    # Start\n",
    "    WordSense(\"décoller\",   RelationDirection.START, RelationStrength.STRONG),\n",
    "    WordSense(\"passer\",     RelationDirection.START, RelationStrength.WEAK),\n",
    "    WordSense(\"être\",       RelationDirection.START, RelationStrength.STRONG),\n",
    "    # Destination\n",
    "    WordSense(\"arriver\",    RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    WordSense(\"aller\",      RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    WordSense(\"visiter\",    RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    WordSense(\"atterrir\",   RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    WordSense(\"découvrir\",  RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    WordSense(\"voyager\",    RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    WordSense(\"rendre\",     RelationDirection.DEST,  RelationStrength.STRONG)\n",
    "]\n",
    "\n",
    "def analyseRequest(request):\n",
    "    print(f\"Request: {request}\")\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    doc = nlp(request)\n",
    "    locations = []\n",
    "    fullTrip = []\n",
    "\n",
    "    # Extract locations\n",
    "    for i in doc.ents:\n",
    "        if i.label_ == 'LOC' or i.label_ == 'GPE': \n",
    "            locations.append(i.text)\n",
    "    print(f\"Locations found: {locations}\")\n",
    "\n",
    "    if len(locations) == 0:\n",
    "        print(\"Sorry, but we cannot answer this request.\")\n",
    "    else:\n",
    "        # Get token for each locations\n",
    "        tokens = np.zeros(len(locations), dtype=object)\n",
    "        for i in range(len(locations)):\n",
    "            tokenFound = False\n",
    "            # Priority: PROPN\n",
    "            for token in doc:\n",
    "                if token.pos == PROPN:\n",
    "                    isUsable = True\n",
    "                    for tokenSelected in tokens:\n",
    "                        if type(tokenSelected) != int and tokenSelected == token:\n",
    "                            isUsable = False\n",
    "                    if isUsable:\n",
    "                        if token.text in locations[i]:\n",
    "                            tokens[i] = token\n",
    "                            tokenFound = True\n",
    "                            break\n",
    "\n",
    "            # Secondary: NOUN\n",
    "            if tokenFound == False:\n",
    "                for token in doc:\n",
    "                    if token.pos == NOUN:\n",
    "                        isUsable = True\n",
    "                        for tokenSelected in tokens:\n",
    "                            if type(tokenSelected) != int and tokenSelected == token:\n",
    "                                isUsable = False\n",
    "                        if isUsable:\n",
    "                            if token.text in locations[i]:\n",
    "                                tokens[i] = token\n",
    "                                tokenFound = True\n",
    "                                break\n",
    "\n",
    "            # Failsafe: any (e.g in \"Je veux faire Paris Gare De l'Est Marseille\": Marseille is parsed as a VERB)\n",
    "            if tokenFound == False:\n",
    "                for token in doc:\n",
    "                    isUsable = True\n",
    "                    for tokenSelected in tokens:\n",
    "                        if type(tokenSelected) != int and tokenSelected == token:\n",
    "                            isUsable = False\n",
    "                    if isUsable:\n",
    "                        if token.text in locations[i]:\n",
    "                            tokens[i] = token\n",
    "                            tokenFound = True\n",
    "                            break\n",
    "\n",
    "            # None\n",
    "            if tokenFound == False:\n",
    "                print(f\"Localization {locations[i]} not found\")\n",
    "                tokens[i] = None\n",
    "\n",
    "        # Remove None tokens\n",
    "        tmpTokens = tokens\n",
    "        tokens = [] \n",
    "        for token in tmpTokens: \n",
    "            if token != None : \n",
    "                tokens.append(token)\n",
    "\n",
    "\n",
    "        # Weight tokens to prepare ordering\n",
    "        weighedTokens = np.zeros(len(tokens), dtype=object)\n",
    "        for i in range(len(tokens)):\n",
    "            print(f\"Token #{i+1} : {tokens[i].lemma_}\")\n",
    "            foundWeight = []\n",
    "            parent = tokens[i].head\n",
    "\n",
    "            # CCONJ\n",
    "            for child in tokens[i].children:\n",
    "                if child.pos == CCONJ:\n",
    "                    for ref in CCONJ_Relation:\n",
    "                        if ref.word == child.lemma_:\n",
    "                            print(f\"Found CCONJ: {ref.word} - {ref.strength.name}\")\n",
    "                            foundWeight.append(ref)\n",
    "                            break\n",
    "\n",
    "            # NOUN\n",
    "            if len(foundWeight) <= 0: # Not prioritary over CCONJ\n",
    "                if parent.pos == NOUN:\n",
    "                    for ref in NOUN_Relation:\n",
    "                        if ref.word == parent.lemma_:\n",
    "                            print(f\"Found NOUN: {ref.word} - {ref.strength.name}\")\n",
    "                            foundWeight.append(ref)\n",
    "                            break\n",
    "\n",
    "            # ADP_FIXED\n",
    "            if len(foundWeight) <= 0: # Not prioritary over CCONJ and NOUN\n",
    "                for child in tokens[i].children:\n",
    "                    if child.pos == ADP:\n",
    "                        for subChild in child.children:\n",
    "                            if subChild.dep_ == 'fixed':\n",
    "                                for ref in ADP_FIXED_Relation:\n",
    "                                    if ref.word == child.lemma_ and ref.fixedWord == subChild.lemma_:\n",
    "                                        print(f\"Found ADP_FIXED: {ref.word} {ref.fixedWord}\")\n",
    "                                        foundWeight.append(ref)\n",
    "                                        break\n",
    "\n",
    "                \n",
    "                    \n",
    "            # ADP\n",
    "            if len(foundWeight) <= 0: # Not prioritary over CCONJ, NOUN and ADP_FIXED\n",
    "                for child in tokens[i].children:\n",
    "                    for ref in ADP_Relation:\n",
    "                        if ref.word == child.lemma_:\n",
    "                            print(f\"Found ADP: {ref.word} - {ref.strength.name}\")\n",
    "                            foundWeight.append(ref)\n",
    "                            break\n",
    "\n",
    "            # VERB_MARK\n",
    "            if len(foundWeight) <= 1: # Prioritary over CCONJ, NOUN and ADP_FIXED\n",
    "                if parent.pos == VERB:\n",
    "                    for child in parent.children:\n",
    "                        if child.dep_ == 'mark' and child.pos == ADP:\n",
    "                            for ref in VERB_MARK_Relation:\n",
    "                                if ref.word == child.lemma_:\n",
    "                                    print(f\"Found VERB_MARK: {ref.word} - {ref.strength.name}\")\n",
    "                                    foundWeight.append(ref)\n",
    "                                    break\n",
    "                \n",
    "            # VERB\n",
    "            if len(foundWeight) <= 1: # Prioritary over CCONJ, NOUN, ADP_FIXED and VERB_MARK\n",
    "                for ref in VERB_Relation:\n",
    "                    if ref.word == parent.lemma_:\n",
    "                        print(f\"Found VERB: {ref.word} - {ref.strength.name}\")\n",
    "                        foundWeight.append(ref)\n",
    "                        break\n",
    "\n",
    "            # Default - Keep position \n",
    "            if len(foundWeight) == 0: # Fallback\n",
    "                print(f\"Using default weight\")\n",
    "                foundWeight.append(WordSense(\"default\", RelationDirection.DEST,  RelationStrength.WEAK))\n",
    "\n",
    "            \n",
    "            # Extract first strong relation\n",
    "            selectedWeight = None\n",
    "            for j in range(len(foundWeight)):\n",
    "                if foundWeight[j].strength == RelationStrength.STRONG:\n",
    "                    selectedWeight = foundWeight[j]\n",
    "                    break\n",
    "            if selectedWeight is None:\n",
    "                selectedWeight = foundWeight[0]\n",
    "\n",
    "            print(f\"Using: {selectedWeight.word}\")\n",
    "            print(\"---------------\")\n",
    "            weighedTokens[i] = (tokens[i], selectedWeight)\n",
    "\n",
    "\n",
    "        # Order tokens\n",
    "        orderedTokens = []\n",
    "        # First pass for direction: START\n",
    "        for i in range(len(weighedTokens)):\n",
    "            token, weight = weighedTokens[i]\n",
    "            numberOfStrongStrength = 0\n",
    "            if weight.direction == RelationDirection.START:\n",
    "                if weight.strength == RelationStrength.STRONG:\n",
    "                    orderedTokens.insert(numberOfStrongStrength, token)\n",
    "                    numberOfStrongStrength = numberOfStrongStrength + 1\n",
    "                else:\n",
    "                    orderedTokens.append(token)\n",
    "        \n",
    "\n",
    "        # Second pass for direction: DEST\n",
    "        numberOfStrongStrength = 0\n",
    "        for i in range(len(weighedTokens)):\n",
    "            token, weight = weighedTokens[i]\n",
    "            if weight.direction == RelationDirection.DEST:\n",
    "                if weight.strength == RelationStrength.STRONG:\n",
    "                    orderedTokens.append(token)\n",
    "                    numberOfStrongStrength = numberOfStrongStrength + 1\n",
    "                else:\n",
    "                    if numberOfStrongStrength == 0:\n",
    "                        orderedTokens.append(token)\n",
    "                    else:\n",
    "                        orderedTokens.insert(len(orderedTokens)-numberOfStrongStrength, token)\n",
    "\n",
    "        # Populate full trip cities list\n",
    "        for token in orderedTokens:\n",
    "            fullTrip.append(token.text)\n",
    "        print(f\"Result trip: {fullTrip}\")\n",
    "\n",
    "        # DEBUG\n",
    "        #for token in doc:\n",
    "        #    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "        #displacy.serve(doc, style=\"dep\")\n",
    "\n",
    "        return fullTrip\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TESTS\n",
    "requests = [\n",
    "    (\"J'aimerais aller d'Orléans à Paris puis dans les Vosges\", [\"Orléans\", \"Paris\", \"Vosges\"]),\n",
    "    (\"Je veux aller à Marseille à partir de Lyon\", [\"Lyon\", \"Marseille\"]),\n",
    "    (\"Je veux visiter Paris en partant de Bordeaux et en passant par Nantes\", [\"Bordeaux\", \"Nantes\", \"Paris\"]),\n",
    "    (\"Je veux prendre le train à Mulhouse à destination de Strasbourg\", [\"Mulhouse\", \"Strasbourg\"]),\n",
    "    (\"Strasbourg en provenance de Mulhouse\", [\"Mulhouse\", \"Strasbourg\"]),\n",
    "    (\"Je veux aller de Mulhouse à Strasbourg\", [\"Mulhouse\", \"Strasbourg\"]),\n",
    "    (\"Je veux faire Paris Gare De l'est Marseille\", [\"Paris\", \"Marseille\"]),\n",
    "    (\"Je veux aller à Paris après être allé à Mulhouse depuis Lyon\", [\"Lyon\", \"Mulhouse\", \"Paris\"]),\n",
    "    (\"Paris-Marseille\", [\"Paris\", \"Marseille\"]),\n",
    "    (\"Je suis à Paris et je veux aller à Strasbourg avec mon amis Frank que je récupère à Mulhouse\", [\"Paris\", \"Mulhouse\", \"Strasbourg\"])\n",
    "]\n",
    "\n",
    "for index in range(len(requests)):\n",
    "    sentence, expectedResult = requests[index]\n",
    "    result = analyseRequest(sentence)\n",
    "    print(f\"\\n\\n\\n***************************    # {index}    ***************************\")\n",
    "    print(f\"result:    {result}\")\n",
    "    print(f\"exprected: {expectedResult}\")\n",
    "    print(\"*****************************************************************\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Request: un smoothie à la banane et une pizza 4 fromages s'il vous plaît\n",
      "Locations found: []\n",
      "Sorry, but we cannot answer this request.\n"
     ]
    }
   ],
   "source": [
    "fullTrip = analyseRequest(userRequest)\n",
    "\n",
    "if fullTrip is not None:\n",
    "    resultTrips = getBestPathForFullTrip(fullTrip)\n",
    "    print(\"\\n\\n\\nHere is the trip you are asking for: \\n\")\n",
    "    for i in range(len(resultTrips)):\n",
    "        if resultTrips[i].path is not None:\n",
    "            print(f\"#{i+1} - {resultTrips[i]}\")\n",
    "        else:\n",
    "            if resultTrips[i].startStationId is None or resultTrips[i].endStationId is None:\n",
    "                print(f\"#{i+1} - Could not find one or both station of the sub-trip\")\n",
    "            else:\n",
    "                print(f\"#{i+1} - Could not find a path between the both stations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}