{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import csv\n",
    "import spacy\n",
    "from enum import Enum\n",
    "from spacy import displacy\n",
    "from spacy.symbols import PROPN, NOUN, CCONJ, ADP, VERB\n",
    "import numpy as np\n",
    "import speech_recognition as sr \n",
    "from scipy.sparse import csr_matrix\n",
    "from sknetwork.path import shortest_path\n",
    "\n",
    "from collections import defaultdict\n",
    "import functools\n",
    "import itertools\n",
    "\n",
    "currentPath = os.path.dirname(os.path.abspath(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Read 3150 go and back trips (1575 distinc trips) out of 1575 rows for 817 distinct train stations.\n"
     ]
    }
   ],
   "source": [
    "# Reading trips from csv file\n",
    "# Inspired from https://stackoverflow.com/a/12398967\n",
    "\n",
    "pathCount = 0\n",
    "timeTableFileName = os.path.join(currentPath, './data/timetables_edited.csv')\n",
    "\n",
    "# Create dictionary to associates a station name with an id\n",
    "trainStationNameToId = defaultdict(functools.partial(next, itertools.count()))\n",
    "\n",
    "with open(timeTableFileName, newline='', encoding='UTF-8') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "    # Set of the reading position (ignoring header line)\n",
    "    csvfile.seek(0)\n",
    "    next(reader)\n",
    "\n",
    "    # First reading to get the number different stations and init shape of trips object\n",
    "    for row in reader:\n",
    "        idxA = trainStationNameToId[row[1]]\n",
    "        idxB = trainStationNameToId[row[2]]\n",
    "    numberOfTrainstations = len(trainStationNameToId)\n",
    "    trips = np.zeros((numberOfTrainstations, numberOfTrainstations))\n",
    "\n",
    "    # Reset of the reading position (ignoring header line)\n",
    "    csvfile.seek(0)\n",
    "    next(reader)\n",
    "\n",
    "    # Reading data\n",
    "    for row in reader:\n",
    "        pathCount += 1 \n",
    "        idxA = trainStationNameToId[row[1]]\n",
    "        idxB = trainStationNameToId[row[2]]\n",
    "\n",
    "        # If trip already exist/has already be read, display message\n",
    "        indexTupple = (idxA, idxB) if idxA < idxB else (idxB, idxA)\n",
    "        if trips[indexTupple] != 0:\n",
    "            print(f\"Trip {row[1]} - {row[2]} with a distance of {row[3]} has already be read with a distance of {trips[indexTupple]}. Ignoring the new one.\")\n",
    "        else:\n",
    "            trips[indexTupple] = int(row[3])\n",
    "\n",
    "\n",
    "# Create dictionarry to map an id to its train station name\n",
    "trainStationIdToName = dict((id, name) for name, id in trainStationNameToId.items())\n",
    "\n",
    "# Make matrix symetrical as the trips are not directed but can be taken in both directions\n",
    "# Source from https://stackoverflow.com/a/42209263\n",
    "i_lower = np.tril_indices(numberOfTrainstations, -1)\n",
    "trips[i_lower] = trips.T[i_lower]\n",
    "\n",
    "# Creating Compressed Sparse Row (CSR) matrix to store and work more efficiently with only the trips\n",
    "tripGraph = csr_matrix(trips)\n",
    "\n",
    "print(f\"Read {tripGraph.getnnz()} go and back trips ({int(tripGraph.getnnz() / 2)} distinc trips) out of {pathCount} rows for {len(trainStationNameToId)} distinct train stations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trip:\n",
    "    def __init__(self, startStationId, endStationId, path, totalDuration):\n",
    "        self.startStationId = startStationId\n",
    "        self.endStationId = endStationId\n",
    "        self.path = path\n",
    "        if totalDuration is None:\n",
    "            self.totalDuration = None\n",
    "        else:\n",
    "            self.totalDuration = int(totalDuration)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Trip from {trainStationIdToName[self.startStationId]} to {trainStationIdToName[self.endStationId]} for a total duration of {self.totalDuration} by this path: {self.path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#1 - Trip from Gare de Orléans to Gare de Paris-Austerlitz for a total duration of 75 by this path: [18, 12]\nGare de Orléans\nGare de Paris-Austerlitz\n#2 - Trip from Gare de Paris-Est to Gare de Strasbourg for a total duration of 287 by this path: [735, 669, 98, 104]\nGare de Paris-Est\nGare de Epernay\nGare de Metz-Ville\nGare de Strasbourg\n#3 - Could not find one or both station of the sub-trip\n#4 - Could not find one or both station of the sub-trip\n#5 - Trip from Gare de Mulhouse to Gare de Mulhouse for a total duration of 0 by this path: [133, 133]\nGare de Mulhouse\nGare de Mulhouse\n"
     ]
    }
   ],
   "source": [
    "# Functions used to determine the shortest path between cities \n",
    "\n",
    "def getPathBetweenIds(trainStationStartIds: list, trainStationEndIds: list):\n",
    "    global tripGraph\n",
    "    paths = []\n",
    "    # If start array contains one element also contained in end array -> return path from/to the same station\n",
    "    for startId in trainStationStartIds:\n",
    "        if startId in trainStationEndIds:\n",
    "            return [int(startId), int(startId)]\n",
    "\n",
    "    # As shortest_path() does not support multiple sources and multiple targets at the same time, we'll iterate through all start points and manually concat the results\n",
    "    if(len(trainStationStartIds) > 1 and len(trainStationEndIds) > 1):\n",
    "        for trainStationEndId in trainStationEndIds:\n",
    "            results = shortest_path(tripGraph, sources=[int(i) for i in trainStationStartIds], targets=[int(trainStationEndId)], method='D')\n",
    "            for result in results:\n",
    "                if len(result) >= 2:\n",
    "                    paths.append(result)\n",
    "        return paths\n",
    "    else:\n",
    "        results = shortest_path(tripGraph, sources=[int(i) for i in trainStationStartIds], targets=[int(i) for i in trainStationEndIds], method='D')\n",
    "        for result in results:\n",
    "            if len(result) >= 2:\n",
    "                paths.append(result)\n",
    "        return paths\n",
    "\n",
    "def isCityMatchingKey(city: str, key: str):\n",
    "    return city.lower() in key.lower()\n",
    "\n",
    "def getPathBetweenCities(start: str, end: str):\n",
    "    trainStationStartIds = np.array([])\n",
    "    trainStationEndIds = np.array([])\n",
    "    \n",
    "    # Get all stations that contains the searched name\n",
    "    for key, value in trainStationNameToId.items():\n",
    "        if isCityMatchingKey(start, key):\n",
    "            trainStationStartIds = np.append(trainStationStartIds, value)\n",
    "        if isCityMatchingKey(end, key):\n",
    "            trainStationEndIds = np.append(trainStationEndIds, value)\n",
    "\n",
    "    if len(trainStationStartIds) > 0 and len(trainStationEndIds) > 0:\n",
    "        return getPathBetweenIds(trainStationStartIds, trainStationEndIds)\n",
    "    else:\n",
    "        return np.array([])\n",
    "\n",
    "def getBestPathForFullTrip(tripCityWaypoints: list):\n",
    "    global tripGraph\n",
    "    fullTrip = np.array(np.zeros(len(tripCityWaypoints)-1), dtype=object)\n",
    "    # Iterate through all sub trips\n",
    "    for trip in range(len(fullTrip)):\n",
    "        paths = getPathBetweenCities(tripCityWaypoints[trip], tripCityWaypoints[trip+1])\n",
    "        minDistance = None\n",
    "        keptPath = None\n",
    "        startId = None\n",
    "        endId = None\n",
    "\n",
    "        # Iterate though the returned array\n",
    "        for path in paths:\n",
    "            distance = 0\n",
    "            # Nested array => multiple start/end possible\n",
    "            if isinstance(path, list):\n",
    "                for i in range(len(path)-1):\n",
    "                    distance = distance + tripGraph[(path[i], path[i+1])]\n",
    "                if minDistance is None or distance < minDistance:\n",
    "                    minDistance = distance\n",
    "                    keptPath = path\n",
    "                    startId = path[0]\n",
    "                    endId = path[len(path)-1]\n",
    "            \n",
    "            # Scalar value => only one path possible\n",
    "            else:\n",
    "                for i in range(len(paths)-1):\n",
    "                    distance = distance + tripGraph[(paths[i], paths[i+1])]\n",
    "                minDistance = distance\n",
    "                keptPath = paths\n",
    "                startId = paths[0]\n",
    "                endId = paths[len(paths)-1]\n",
    "\n",
    "        fullTrip[trip] = Trip(startId, endId, keptPath, minDistance)\n",
    "    return fullTrip\n",
    "\n",
    "\n",
    "# Use following to test pathfinding functions above\n",
    "def testPathfinding():\n",
    "    bestTrips = getBestPathForFullTrip(['Orléan', 'Paris', 'Strasbourg', 'dsfdsfd', 'Mulhouse', 'Mulhouse'])\n",
    "    for i in range(len(bestTrips)):\n",
    "        if bestTrips[i].path is not None:\n",
    "            print(f\"#{i+1} - {bestTrips[i]}\")\n",
    "            for stationId in bestTrips[i].path:\n",
    "                print(f\"{trainStationIdToName[stationId]}\")\n",
    "        else:\n",
    "            if bestTrips[i].startStationId is None or bestTrips[i].endStationId is None:\n",
    "                print(f\"#{i+1} - Could not find one or both station of the sub-trip\")\n",
    "            else:\n",
    "                print(f\"#{i+1} - Could not find a path between the both stations\")\n",
    "testPathfinding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Adjusting to noise level...\n",
      "Listening...\n",
      "Voice recognition...\n",
      "Parsed: 'j'aimerais aller de Mulhouse à Paris'\n"
     ]
    }
   ],
   "source": [
    "# Getting user's request\n",
    "# Inspired from https://www.geeksforgeeks.org/python-convert-speech-to-text-and-text-to-speech/\n",
    "try: \n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source: \n",
    "        print(\"Adjusting to noise level...\")\n",
    "        r.adjust_for_ambient_noise(source, duration=0.2) \n",
    " \n",
    "        print(\"Listening...\")\n",
    "        audio = r.listen(source) \n",
    "            \n",
    "        print(\"Voice recognition...\")\n",
    "        request = r.recognize_google(audio, language=\"fr-FR\") \n",
    "\n",
    "        print(f\"Parsed: '{request}'\") \n",
    "            \n",
    "except sr.RequestError as e: \n",
    "    print(f\"Exception during request parsing: {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Request: Je veux visiter Paris en partant de Bordeaux et en passant par Nantes\n",
      "Locations found: ['Paris', 'Bordeaux', 'Nantes']\n",
      "Result trip: ['Bordeaux', 'Paris', 'Nantes']\n"
     ]
    }
   ],
   "source": [
    "# Extract cities list in the right travel order from the user's request\n",
    "\n",
    "# Spacy documentation: https://spacy.io/usage/linguistic-features\n",
    "# NLP Feature that we use or can be useful:\n",
    "#   - Lemmatization: we use it to avoid defining every possible form of a word when defining our rules\n",
    "#   - PoS Tagging: we use it to detect the function of a word which helps us to choose which rule we apply and reduce the number of checks we need to do\n",
    "#   - Dependency Parsing: we use it to navigate in the sentence between the words that are linked\n",
    "#   - Word Senses: we use it to define our rules that assigns a specific direction and strength to apply to certain words/fixed word groups\n",
    "#   - Constituent Parsing: we currently don't use it, but it might help us improve reliability by splitting sentences into sub-sentences that we can then organize between them\n",
    "\n",
    "\n",
    "# Examples:\n",
    "#request = \"J'aimerais aller d'Orléans à Paris puis dans les Vosges\"\n",
    "#request = \"Je veux aller à Marseille à partir de Lyon\"\n",
    "request = \"Je veux visiter Paris en partant de Bordeaux et en passant par Nantes\"\n",
    "#request = \"Je veux prendre le train à Mulhouse à destination de Strasbourg\"\n",
    "#request = \"Strasbourg en provenance de Mulhouse\"\n",
    "#request = \"Je veux aller de Mulhouse à Strasbourg\"\n",
    "#request = \"Je veux faire Paris Gare De l'est Marseil\"\n",
    "#request = \"Je veux aller à Paris après être allé à Mulhouse depuis Lyon\" #TODO find a way to make this work without breaking the rest as the structure is quite tricky (would Constituent Parsing help here?)\n",
    "#request = \"Paris Marseille\" #TODO Does not pass - Issue here is that spacy does not recognize Marseille as a city, maybe try with fr_core_news_md or fr_core_news_lg (also check if the performances are still the sames)\n",
    "\n",
    "print(f\"Request: {request}\")\n",
    "\n",
    "class RelationDirection(Enum):\n",
    "    NONE = 1\n",
    "    START = 2\n",
    "    DEST = 3\n",
    "\n",
    "class RelationStrength(Enum):\n",
    "    NONE = 1\n",
    "    WEAK = 2\n",
    "    STRONG = 3\n",
    "\n",
    "\n",
    "class WordSense:\n",
    "    def __init__(self, word: str, direction: RelationDirection, strength: RelationStrength):\n",
    "        self.word = word\n",
    "        self.direction = direction\n",
    "        self.strength = strength\n",
    "\n",
    "    def __str__(self):\n",
    "     return f\"Word '{self.word}' has a direction of {self.direction.name} and a {self.strength.name} strength.\"\n",
    "\n",
    "class LinkedWordSense:\n",
    "    def __init__(self, word: str, fixedWord: str, direction: RelationDirection, strength: RelationStrength):\n",
    "        self.word = word\n",
    "        self.fixedWord = fixedWord\n",
    "        self.direction = direction\n",
    "        self.strength = strength\n",
    "\n",
    "    def __str__(self):\n",
    "     return f\"Words '{self.word}' fixed with '{self.fixedWord}' has a direction of {self.direction.name} and a {self.strength.name} strength.\"\n",
    "\n",
    "# CCONJ links: 'cc'_child\n",
    "CCONJ_Relation = [\n",
    "    # Start\n",
    "    WordSense(\"depuis\",     RelationDirection.START, RelationStrength.STRONG),\n",
    "    # Destination\n",
    "    WordSense(\"puis\",       RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    WordSense(\"et\",         RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    WordSense(\"enfin\",      RelationDirection.DEST,  RelationStrength.STRONG)\n",
    "]\n",
    "\n",
    "# NOUN links: 'nmod'_parent\n",
    "NOUN_Relation = [\n",
    "    # Start\n",
    "    WordSense(\"provenance\",     RelationDirection.START, RelationStrength.STRONG),\n",
    "    # Destination\n",
    "    WordSense(\"direction\",      RelationDirection.DEST,  RelationStrength.WEAK),\n",
    "    WordSense(\"destination\",    RelationDirection.DEST,  RelationStrength.WEAK)\n",
    "]\n",
    "\n",
    "# ADP_FIXED has the priority \n",
    "# ADP links: 'case'_child, 'dep'_parent\n",
    "ADP_FIXED_Relation = [\n",
    "    # Start\n",
    "    LinkedWordSense(\"à\",\"partir\",       RelationDirection.START, RelationStrength.STRONG),\n",
    "    LinkedWordSense(\"en\", \"partant\",    RelationDirection.START, RelationStrength.STRONG),\n",
    "    # Destination\n",
    "    LinkedWordSense(\"à\",\"destination\",  RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    LinkedWordSense(\"en\",\"direction\",   RelationDirection.DEST,  RelationStrength.WEAK)\n",
    "]\n",
    "ADP_Relation = [\n",
    "    # Start\n",
    "    WordSense(\"de\",     RelationDirection.START, RelationStrength.STRONG),\n",
    "    WordSense(\"du\",     RelationDirection.START, RelationStrength.STRONG),\n",
    "    WordSense(\"des\",    RelationDirection.START, RelationStrength.STRONG),\n",
    "    # Destination\n",
    "    WordSense(\"à\",      RelationDirection.DEST,  RelationStrength.WEAK),\n",
    "    WordSense(\"au\",     RelationDirection.DEST,  RelationStrength.WEAK),\n",
    "    WordSense(\"aux\",    RelationDirection.DEST,  RelationStrength.WEAK),\n",
    "    WordSense(\"dans\",   RelationDirection.DEST,  RelationStrength.WEAK),\n",
    "    WordSense(\"en\",     RelationDirection.DEST,  RelationStrength.WEAK),\n",
    "    WordSense(\"par\",    RelationDirection.DEST,  RelationStrength.WEAK) # par : \"passer par Paris\"\n",
    "] \n",
    "\n",
    "# VERB links: 'obl:arg'_parent, 'obl:mod'_parent\n",
    "# \"partir\" is ambiguous: \"partir de ...\" \"partir à ...\"\n",
    "VERB_Relation = [\n",
    "    # Start\n",
    "    WordSense(\"décoller\",   RelationDirection.START, RelationStrength.STRONG),\n",
    "    WordSense(\"passer\",   RelationDirection.START, RelationStrength.WEAK),\n",
    "    # Destination\n",
    "    WordSense(\"arriver\",    RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    WordSense(\"aller\",      RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    WordSense(\"visiter\",    RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    WordSense(\"atterrir\",   RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    WordSense(\"découvrir\",  RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    WordSense(\"voyager\",    RelationDirection.DEST,  RelationStrength.STRONG),\n",
    "    WordSense(\"rendre\",     RelationDirection.DEST,  RelationStrength.STRONG)\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(request)\n",
    "locations = []\n",
    "fullTrip = []\n",
    "\n",
    "# Extract locations\n",
    "for i in doc.ents:\n",
    "    if i.label_ == 'LOC' or i.label_ == 'GPE': \n",
    "        locations.append(i.text)\n",
    "print(f\"Locations found: {locations}\")\n",
    "\n",
    "if len(locations) == 0:\n",
    "    print(\"Sorry, but we cannot answer this request.\")\n",
    "else:\n",
    "    # Get token for each locations\n",
    "    tokens = np.zeros(len(locations), dtype=object)\n",
    "    for i in range(len(locations)):\n",
    "        tokenFound = False\n",
    "        # Priority: PROPN\n",
    "        for token in doc:\n",
    "            if token.pos == PROPN:\n",
    "                isUsable = True\n",
    "                for tokenSelected in tokens:\n",
    "                    if type(tokenSelected) != int and tokenSelected == token:\n",
    "                        isUsable = False\n",
    "                if isUsable:\n",
    "                    if token.text in locations[i]:\n",
    "                        tokens[i] = token\n",
    "                        tokenFound = True\n",
    "                        break\n",
    "\n",
    "        # Secondary: NOUN\n",
    "        if tokenFound == False:\n",
    "             for token in doc:\n",
    "                if token.pos == NOUN:\n",
    "                    isUsable = True\n",
    "                    for tokenSelected in tokens:\n",
    "                        if type(tokenSelected) != int and tokenSelected == token:\n",
    "                            isUsable = False\n",
    "                    if isUsable:\n",
    "                        if token.text in locations[i]:\n",
    "                            tokens[i] = token\n",
    "                            tokenFound = True\n",
    "                            break\n",
    "\n",
    "        # Failsafe: any (e.g in \"Je veux faire Paris Gare De l'Est Marseil\" marseille is parsed as a VERB)\n",
    "        if tokenFound == False:\n",
    "            for token in doc:\n",
    "                isUsable = True\n",
    "                for tokenSelected in tokens:\n",
    "                    if type(tokenSelected) != int and tokenSelected == token:\n",
    "                        isUsable = False\n",
    "                if isUsable:\n",
    "                    if token.text in locations[i]:\n",
    "                        tokens[i] = token\n",
    "                        tokenFound = True\n",
    "                        break\n",
    "\n",
    "        # None\n",
    "        if tokenFound == False:\n",
    "            print(f\"Localization {locations[i]} not found\")\n",
    "            tokens[i] = None\n",
    "\n",
    "    # Remove None tokens\n",
    "    tmpTokens = tokens\n",
    "    tokens = [] \n",
    "    for token in tmpTokens: \n",
    "        if token != None : \n",
    "            tokens.append(token)\n",
    "\n",
    "\n",
    "    # Weight tokens to prepare ordering\n",
    "    weighedTokens = np.zeros(len(tokens), dtype=object)\n",
    "    for i in range(len(tokens)):\n",
    "\n",
    "        # If any CCONJ => use it\n",
    "        cconjFound = False\n",
    "        for child in tokens[i].children:\n",
    "            if cconjFound == True:\n",
    "                break\n",
    "            if child.pos == CCONJ:\n",
    "                for ref in CCONJ_Relation:\n",
    "                    if ref.word == child.lemma_:\n",
    "                        cconjFound = True\n",
    "                        weighedTokens[i] = (tokens[i], ref)\n",
    "                        break\n",
    "\n",
    "        # Else try NOUN\n",
    "        if cconjFound == False:\n",
    "            nounFound = False\n",
    "            parent = tokens[i].head\n",
    "            if parent.pos == NOUN:\n",
    "                for ref in NOUN_Relation:\n",
    "                    if ref.word == parent.lemma_:\n",
    "                        nounFound = True\n",
    "                        weighedTokens[i] = (tokens[i], ref)\n",
    "                        break\n",
    "\n",
    "            # Else try ADP\n",
    "            if nounFound == False:\n",
    "                adpFound = False\n",
    "                # If any ADP_FIXED => use it\n",
    "                for child in tokens[i].children:\n",
    "                    if adpFound == True:\n",
    "                        break\n",
    "                    if child.pos == ADP:\n",
    "                        for subChild in child.children:\n",
    "                            if subChild.dep_ == 'fixed':\n",
    "                                for ref in ADP_FIXED_Relation:\n",
    "                                    if ref.word == child.lemma_ and ref.fixedWord == subChild.lemma_:\n",
    "                                        adpFound = True\n",
    "                                        weighedTokens[i] = (tokens[i], ref)\n",
    "                                        break\n",
    "                # Else if any ADP => use it\n",
    "                if adpFound == False:\n",
    "                    for child in tokens[i].children:\n",
    "                        if adpFound == True:\n",
    "                            break\n",
    "                        for ref in ADP_Relation:\n",
    "                            if ref.word == child.lemma_:\n",
    "                                adpFound = True\n",
    "                                weighedTokens[i] = (tokens[i], ref)\n",
    "                                break\n",
    "            \n",
    "                # Else if any VERB => use it\n",
    "                if adpFound == False:\n",
    "                    verbFound = False\n",
    "                    parent = tokens[i].head\n",
    "                    if parent.pos == VERB:\n",
    "                        for ref in VERB_Relation:\n",
    "                            if ref.word == parent.lemma_:\n",
    "                                verbFound = True\n",
    "                                weighedTokens[i] = (tokens[i], ref)\n",
    "                                break\n",
    "\n",
    "                    # Else Keep position \n",
    "                    if verbFound == False:\n",
    "                        weighedTokens[i] = (tokens[i], WordSense(\"\", RelationDirection.DEST,  RelationStrength.WEAK))\n",
    "    \n",
    "\n",
    "    # Order tokens\n",
    "    orderedTokens = []\n",
    "    # First pass for direction: START\n",
    "    for i in range(len(weighedTokens)):\n",
    "        token, weight = weighedTokens[i]\n",
    "        numberOfStrongStrength = 0\n",
    "        if weight.direction == RelationDirection.START:\n",
    "            if weight.strength == RelationStrength.STRONG:\n",
    "                orderedTokens.insert(numberOfStrongStrength, token)\n",
    "                numberOfStrongStrength = numberOfStrongStrength + 1\n",
    "            else:\n",
    "                orderedTokens.append(token)\n",
    "    \n",
    "\n",
    "    # Second pass for direction: DEST\n",
    "    for i in range(len(weighedTokens)):\n",
    "        token, weight = weighedTokens[i]\n",
    "        numberOfStrongStrength = 0\n",
    "        if weight.direction == RelationDirection.DEST:\n",
    "            if weight.strength == RelationStrength.STRONG:\n",
    "                orderedTokens.append(token)\n",
    "                numberOfStrongStrength = numberOfStrongStrength + 1\n",
    "            else:\n",
    "                if numberOfStrongStrength == 0:\n",
    "                    orderedTokens.append(token)\n",
    "                else:\n",
    "                    orderedTokens.insert(len(numberOfStrongStrength)-numberOfStrongStrength, token)\n",
    "\n",
    "    # Populate full trip cities list\n",
    "    for token in orderedTokens:\n",
    "        fullTrip.append(token.text)\n",
    "    print(f\"Result trip: {fullTrip}\")\n",
    "\n",
    "# DEBUG\n",
    "#for token in doc:\n",
    "#    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "\n",
    "#displacy.serve(doc, style=\"dep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#1 - Trip from Gare de Bordeaux-St-Jean to Gare de Paris-Austerlitz for a total duration of 375 by this path: [605, 676, 13, 12]\nGare de Bordeaux-St-Jean\nGare de Limoges-Bénédictins\nGare de Vierzon\nGare de Paris-Austerlitz\n#2 - Trip from Gare de Paris-Austerlitz to Gare de Nantes for a total duration of 216 by this path: [12, 21, 19, 408]\nGare de Paris-Austerlitz\nGare de St-Pierre-des-Corps\nGare de Tours\nGare de Nantes\n"
     ]
    }
   ],
   "source": [
    "resultTrips = getBestPathForFullTrip(fullTrip)\n",
    "for i in range(len(resultTrips)):\n",
    "    if resultTrips[i].path is not None:\n",
    "        print(f\"#{i+1} - {resultTrips[i]}\")\n",
    "        for stationId in resultTrips[i].path:\n",
    "            print(f\"{trainStationIdToName[stationId]}\")\n",
    "    else:\n",
    "        if resultTrips[i].startStationId is None or resultTrips[i].endStationId is None:\n",
    "            print(f\"#{i+1} - Could not find one or both station of the sub-trip\")\n",
    "        else:\n",
    "            print(f\"#{i+1} - Could not find a path between the both stations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}